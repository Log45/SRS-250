@ARTICLE{Ivakhnenko1968-is,
  title   = "The Group Method of Data {Handling-A} Rival of the Method of
             Stochastic Approximation",
  author  = "Ivakhnenko, A G",
  journal = "Soviet Automatic Control",
  volume  =  1,
  number  =  3,
  pages   = "43--55",
  year    =  1968
}

@ARTICLE{Amari1993-eb,
  title     = "Backpropagation and stochastic gradient descent method",
  author    = "Amari, Shun-Ichi",
  abstract  = "The backpropagation learning method has opened a way to wide
               applications of neural network research. It is a type of the
               stochastic descent method known in the sixties. The present
               paper reviews the wide applicability of the stochastic gradient
               descent method to various types of models and loss functions. In
               particular, we apply it to the pattern recognition problem,
               obtaining a new learning algorithm based on the information
               criterion. Dynamical properties of learning curves are then
               studied based on an old paper by the author where the stochastic
               descent method was proposed for general multilayer networks. The
               paper is concluded with a short section offering some historical
               remarks.",
  journal   = "Neurocomputing",
  publisher = "Elsevier BV",
  volume    =  5,
  number    = "4-5",
  pages     = "185--196",
  month     =  jun,
  year      =  1993,
  language  = "en"
}

@ARTICLE{Haghrah2019-tk,
  title   = "Simple {MLP} Backpropagation Artificial Neural Network in C++
             (Step by Step)",
  author  = "Haghrah, Amiraslan",
  journal = "CodeProject, CodeProject",
  year    =  2019
}

@BOOK{Minsky1969-td,
  title     = "Perceptrons",
  author    = "Minsky, Marvin and Papert, Seymour",
  publisher = "MIT Press",
  month     =  jun,
  year      =  1969,
  address   = "London, England",
  language  = "en"
}

@BOOK{Rosenblatt1958-mp,
  title  = "The Perceptron: A Theory of Statistical Separability in Cognitive
            Systems (Project Para)",
  author = "Rosenblatt, Frank",
  year   =  1958
}

@ARTICLE{Vaswani2017-ri,
  title         = "Attention is all you need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}
